{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "494799f9-6d45-4f11-9b90-778dccf17a4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import glob\n",
    "import warnings\n",
    "import datetime\n",
    "import scipy.stats\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import xarray as xr\n",
    "import xskillscore as xs\n",
    "import pandas as pd\n",
    "import xesmf as xesmf\n",
    "import operator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7de85128-a2a0-44f6-ad32-6be514769379",
   "metadata": {},
   "outputs": [],
   "source": [
    "def pnw_average(dataArray,latstr,lonstr):\n",
    "    #Compute the latitude weighted average over the PNW\n",
    "    datout = dataArray.where((dataArray[latstr] > 40.5) & (dataArray[latstr] < 50.5) &\n",
    "                            (dataArray[lonstr] > 235.5) & (dataArray[lonstr] < 253.5),drop=True)\n",
    "    weights = np.cos(np.deg2rad(datout[latstr]))\n",
    "    weights.name = 'weights'\n",
    "    datout_weighted = datout.weighted(weights).mean(dim=[latstr,lonstr],skipna=True)\n",
    "    return datout_weighted\n",
    "\n",
    "def regional_average(dataArray,lonmin,lonmax,lonstr,latmin,latmax,latstr):\n",
    "    #Compute the latitude weighted average over the region of interest\n",
    "    datout = dataArray.where((dataArray[latstr] > 40.5) & (dataArray[latstr] < 50.5) &\n",
    "                            (dataArray[lonstr] > 235.5) & (dataArray[lonstr] < 253.5),drop=True)\n",
    "    weights = np.cos(np.deg2rad(datout[latstr]))\n",
    "    weights.name = 'weights'\n",
    "    datout_weighted = datout.weighted(weights).mean(dim=[latstr,lonstr],skipna=True)\n",
    "    return datout_weighted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6fa8638a-3fc8-430f-b99d-2bcbcb294d44",
   "metadata": {},
   "outputs": [],
   "source": [
    "def coordnames(nci):\n",
    "    if 'latitude' in list(nci.variables):\n",
    "        latstr = 'latitude'\n",
    "    elif 'lat' in list(nci.variables):\n",
    "        latstr = 'lat'\n",
    "    elif 'nav_lat' in list(nci.variables):\n",
    "        latstr = 'nav_lat'\n",
    "\n",
    "    if 'longitude' in list(nci.variables):\n",
    "        lonstr = 'longitude'\n",
    "    elif 'lon' in list(nci.variables):\n",
    "        lonstr = 'lon'\n",
    "    elif 'nav_lon' in list(nci.variables):\n",
    "        lonstr = 'nav_lon'\n",
    "\n",
    "    if 'lat' in list(nci.dims):\n",
    "        latdim = 'lat'\n",
    "        londim = 'lon'\n",
    "    elif 'latitude' in list(nci.dims):\n",
    "        latdim = 'latitude'\n",
    "        londim = 'longitude'\n",
    "    elif 'nav_lat' in list(nci.dims):\n",
    "        latdim = 'nav_lat'\n",
    "        londim = 'nav_lon'\n",
    "    elif 'x' in list(nci.dims):\n",
    "        latdim = 'y'\n",
    "        londim = 'x'\n",
    "    else:\n",
    "        latdim = 'j'\n",
    "        londim = 'i'\n",
    "    return latstr,lonstr,latdim,londim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c841a0f-60a2-49eb-a983-6e9eff77b73d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#lil function to alter longitude to span [0,360] if it spans [-180,180]\n",
    "def fixlons(nci,latdim,londim,lonstr):\n",
    "    lonarray = np.zeros(nci[lonstr].shape)\n",
    "    ndim = lonarray.ndim\n",
    "    if ndim == 2:\n",
    "        if float(nci[lonstr].min()) < -1.:\n",
    "            for i in range(nci[lonstr].shape[0]):\n",
    "                for j in range(nci[lonstr].shape[1]):\n",
    "                    if float(nci[lonstr][i,j]) < 0.:\n",
    "                        lonarray[i,j] = nci[lonstr][i,j].data + 360.\n",
    "                    else:\n",
    "                        lonarray[i,j] = nci[lonstr][i,j].data\n",
    "            nci[lonstr] = ([latdim, londim], lonarray)\n",
    "    elif ndim == 1:\n",
    "        if float(nci[lonstr].min()) < -1.:\n",
    "            for i in range(nci[lonstr].shape[0]):\n",
    "                if float(nci[lonstr][i]) < 0.:\n",
    "                    lonarray[i] = nci[lonstr][i].data + 360.\n",
    "                else:\n",
    "                    lonarray[i] = nci[lonstr][i].data\n",
    "            nci[lonstr] = ([londim], lonarray)\n",
    "    return nci"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94063944-a0b3-4ece-a813-39359292ad06",
   "metadata": {},
   "outputs": [],
   "source": [
    "#computes the linear trend of precipitation and temperature (only used for SSP calculations)\n",
    "def compute_trend(nci):\n",
    "    #this bit is just because xarray doesn't recognize single valued dimensions\n",
    "    if 'ens' in nci.variables:\n",
    "        nens = len(nci['ens'])\n",
    "    else:\n",
    "        nens = 1\n",
    "    month_length = nci.time.dt.days_in_month\n",
    "    tmp = (nci['pr']*month_length)/10\n",
    "\n",
    "    p = pnw_average(tmp,'lat','lon')\n",
    "    p = p.groupby('time.year').sum(dim='time',skipna=False)\n",
    "    t = pnw_average(nci['tas'],'lat','lon')\n",
    "    t = t.groupby('time.year').mean(dim='time')\n",
    "    ptmp = xs.linslope(p['year'],p,dim='year',skipna=False)*100\n",
    "    ttmp = xs.linslope(t['year'],t,dim='year',skipna=False)*100\n",
    "    ptrend = np.nanmean(ptmp)\n",
    "    ttrend = np.nanmean(ttmp)\n",
    "    \n",
    "    return ptrend,ttrend,ptmp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "378be672-8ce3-4b09-8e2e-838a5c640392",
   "metadata": {},
   "outputs": [],
   "source": [
    "def seasonal_avg_vars(nci,model,latstr,lonstr,obs):\n",
    "# Computes seasonal averages for pr, tas, eli, and n34\n",
    "    years = list(nci.groupby('time.year').groups)\n",
    "    nyr = len(years)\n",
    "    seaskeys = ['DJF','MAM','JJA','SON']\n",
    "    drs = {}\n",
    "    if obs == False:\n",
    "        ens = list(nci['ens'])\n",
    "        nens = len(ens)\n",
    "    for seas in seaskeys:\n",
    "        drs[seas] = {}\n",
    "    for iy in range(nyr-1):\n",
    "        drs['DJF'][iy] = slice(str(years[iy])+'-12-01',str(years[iy+1])+'-02-28')\n",
    "        drs['MAM'][iy] = slice(str(years[iy+1])+'-03-01',str(years[iy+1])+'-05-30')\n",
    "        drs['JJA'][iy] = slice(str(years[iy+1])+'-06-01',str(years[iy+1])+'-08-30')\n",
    "        drs['SON'][iy] = slice(str(years[iy+1])+'-09-01',str(years[iy+1])+'-11-30')\n",
    "\n",
    "    nci['pranom'] = nci['pr'].groupby('time.month') - nci['pr'].groupby('time.month').mean(dim='time')\n",
    "    nci['tasanom'] = nci['tas'].groupby('time.month') - nci['tas'].groupby('time.month').mean(dim='time')\n",
    "    outvars = {}\n",
    "    eli = {}\n",
    "    pr = {}\n",
    "    pranom = {}\n",
    "    tas = {}\n",
    "    tasanom = {}\n",
    "    n34 = {}\n",
    "    if obs == False:\n",
    "        for seas in seaskeys:\n",
    "            eli[seas] = np.zeros((nens,nyr-1))\n",
    "            n34[seas] = np.zeros((nens,nyr-1))\n",
    "            pr[seas]  = np.zeros((nens,nyr-1,len(nci[latstr]),len(nci[lonstr])))\n",
    "            pranom[seas]  = np.zeros((nens,nyr-1,len(nci[latstr]),len(nci[lonstr])))\n",
    "            tas[seas] = np.zeros((nens,nyr-1,len(nci[latstr]),len(nci[lonstr])))\n",
    "            tasanom[seas] = np.zeros((nens,nyr-1,len(nci[latstr]),len(nci[lonstr])))\n",
    "\n",
    "            for iy in range(nyr-1):\n",
    "                eli[seas][:,iy]     = nci['eli'].sel(time=drs[seas][iy]).mean(dim='time').values\n",
    "                n34[seas][:,iy]     = nci['n34'].sel(time=drs[seas][iy]).mean(dim='time').values\n",
    "                pranom[seas][:,iy,:,:]  = nci['pranom'].sel(time=drs[seas][iy]).mean(dim='time').values\n",
    "                tasanom[seas][:,iy,:,:] = nci['tasanom'].sel(time=drs[seas][iy]).mean(dim='time').values\n",
    "                pr[seas][:,iy,:,:]      = nci['pr'].sel(time=drs[seas][iy]).mean(dim='time').values\n",
    "                tas[seas][:,iy,:,:]     = nci['tas'].sel(time=drs[seas][iy]).mean(dim='time').values\n",
    "\n",
    "            outvars[seas] = xr.Dataset(\n",
    "                data_vars = dict(\n",
    "                    eli=(['ens','time'], eli[seas]),\n",
    "                    n34=(['ens','time'], n34[seas]),\n",
    "                    pr=(['ens','time','lat','lon'],pr[seas]),\n",
    "                    pranom=(['ens','time','lat','lon'],pranom[seas]),\n",
    "                    tas=(['ens','time','lat','lon'],tas[seas]),\n",
    "                    tasanom=(['ens','time','lat','lon'],tasanom[seas]),\n",
    "                ),\n",
    "                coords = dict(\n",
    "                    ens = (['ens'], nci['ens'].data),\n",
    "                    time = pd.date_range(str(years[0]+1)+'-01-01', periods=nyr-1, freq='AS'),\n",
    "                    lat = (['lat'],nci[latstr].data),\n",
    "                    lon = (['lon'],nci[lonstr].data),\n",
    "                ),\n",
    "                attrs=dict(description= seas + ' average variables from: ' + model),\n",
    "            )\n",
    "    elif obs == True:\n",
    "        for seas in seaskeys:\n",
    "            eli[seas] = np.zeros(nyr-1)\n",
    "            n34[seas] = np.zeros(nyr-1)\n",
    "            pr[seas]  = np.zeros((nyr-1,len(nci[latstr]),len(nci[lonstr])))\n",
    "            pranom[seas]  = np.zeros((nyr-1,len(nci[latstr]),len(nci[lonstr])))\n",
    "            tas[seas] = np.zeros((nyr-1,len(nci[latstr]),len(nci[lonstr])))\n",
    "            tasanom[seas] = np.zeros((nyr-1,len(nci[latstr]),len(nci[lonstr])))\n",
    "\n",
    "            for iy in range(nyr-1):\n",
    "                eli[seas][iy]           = nci['eli'].sel(time=drs[seas][iy]).mean(dim='time').values\n",
    "                n34[seas][iy]           = nci['n34'].sel(time=drs[seas][iy]).mean(dim='time').values\n",
    "                pranom[seas][iy,:,:]    = nci['pranom'].sel(time=drs[seas][iy]).mean(dim='time').values\n",
    "                tasanom[seas][iy,:,:]   = nci['tasanom'].sel(time=drs[seas][iy]).mean(dim='time').values\n",
    "                pr[seas][iy,:,:]        = nci['pr'].sel(time=drs[seas][iy]).mean(dim='time').values\n",
    "                tas[seas][iy,:,:]       = nci['tas'].sel(time=drs[seas][iy]).mean(dim='time').values\n",
    "\n",
    "            outvars[seas] = xr.Dataset(\n",
    "                data_vars = dict(\n",
    "                    eli=(['time'], eli[seas]),\n",
    "                    n34=(['time'], n34[seas]),\n",
    "                    pr=(['time','lat','lon'],pr[seas]),\n",
    "                    pranom=(['time','lat','lon'],pranom[seas]),\n",
    "                    tas=(['time','lat','lon'],tas[seas]),\n",
    "                    tasanom=(['time','lat','lon'],tasanom[seas]),\n",
    "                ),\n",
    "                coords = dict(\n",
    "                    time = pd.date_range(str(years[0]+1)+'-01-01', periods=nyr-1, freq='AS'),\n",
    "                    lat = (['lat'],nci[latstr].data),\n",
    "                    lon = (['lon'],nci[lonstr].data),\n",
    "                ),\n",
    "                attrs=dict(description= seas + ' average variables from: ' + model),\n",
    "            )\n",
    "    return outvars"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e4d81a3-1761-406e-90ac-6f513878d7b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reading in CMIP6 datasets and parsing model/variant information\n",
    "diri='/glade/u/home/nlybarger/scratch/data/climate_data/cmip6/postproc/'\n",
    "filis = sorted(glob.glob(diri + '*historical.nc'))\n",
    "nfil = len(filis)\n",
    "models = ['0']*nfil\n",
    "for i in range(nfil):\n",
    "    models[i] = filis[i].split('/')[-1].split('.')[0]\n",
    "\n",
    "i=0\n",
    "nci = {}\n",
    "variants = {}\n",
    "for fil in filis:\n",
    "    nci[models[i]] = xr.open_dataset(fil,engine='netcdf4')\n",
    "    variants[models[i]] = list(nci[models[i]]['ens'].data)\n",
    "    i += 1\n",
    "firstrun = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "211474b9-8c4e-41e4-a8db-cdf8ca80f84f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read in observational datasets\n",
    "\n",
    "odiri = '/glade/work/nlybarger/data/OBS/'\n",
    "odsets = ['CRU','ERA-5','GMET','UDel','Livneh','PRISM']\n",
    "fnames = ['cru','era5','gmetensm','udel','livneh','prism',]\n",
    "obs = {}\n",
    "oyears = {}\n",
    "for i in range(len(odsets)):\n",
    "    obs[odsets[i]] = xr.open_dataset(odiri + odsets[i] + '/1deg.' + fnames[i] + '.wconus.p.t.nc',engine='netcdf4')\n",
    "    oyears[odsets[i]] = list(obs[odsets[i]].groupby('time.year').groups)\n",
    "\n",
    "n34f = '/glade/work/nlybarger/data/clim_indices/nino34.1870-2021.txt'\n",
    "fp = open(n34f,'r')\n",
    "n34o = np.genfromtxt(fp,delimiter=',',usecols=np.arange(1,13),dtype='f4')\n",
    "n34o = np.reshape(n34o[30:150,:],(120*12))\n",
    "fp.close()\n",
    "\n",
    "elifi = '/glade/work/nlybarger/data/clim_indices/ELI_ERSSTv5_1854.01-2019.12.csv'\n",
    "fp = open(elifi,'r')\n",
    "elio = np.genfromtxt(fp,delimiter=',',usecols=np.arange(47,167),dtype='f4',skip_header=1)\n",
    "elio = np.transpose(elio)\n",
    "elio = np.reshape(elio,(120*12,))\n",
    "fp.close()\n",
    "\n",
    "indy = xr.Dataset(\n",
    "        data_vars = dict(\n",
    "            eli=(['time'], elio),\n",
    "            n34=(['time'], n34o),\n",
    "        ),\n",
    "        coords = dict(\n",
    "            time=(['time'], pd.date_range('1900-01-01','2019-12-31',freq='MS')),\n",
    "        ),\n",
    ")\n",
    "\n",
    "oseasvars = {}\n",
    "for dset in odsets:\n",
    "    print(dset)\n",
    "    if dset in ['CRU','ERA-5','GMET','PRISM']:\n",
    "        oyears[dset] = oyears[dset][:-2]\n",
    "\n",
    "    obs[dset] = obs[dset].sel(time=slice(str(oyears[dset][0])+'-01-01',str(oyears[dset][-1])+'-12-31'))\n",
    "    obs[dset]['n34'] = (['time'],indy['n34'].sel(time=slice(str(oyears[dset][0])+'-01-01',str(oyears[dset][-1])+'-12-31')).data)\n",
    "    obs[dset]['eli'] = (['time'],indy['eli'].sel(time=slice(str(oyears[dset][0])+'-01-01',str(oyears[dset][-1])+'-12-31')).data)\n",
    "    oseasvars[dset] = seasonal_avg_vars(obs[dset],'obs','lat','lon',True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52466c49-639f-4bae-bf83-aeec9e1d8abb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute metrics for each observational dataset\n",
    "\n",
    "dmeanto = {}\n",
    "dsampto = {}\n",
    "dmeanpo = {}\n",
    "dsamppo = {}\n",
    "ddjf_corrs_obs = {}\n",
    "dptrendo = {}\n",
    "dttrendo = {}\n",
    "\n",
    "for dset in odsets:\n",
    "    nyr = len(oyears[dset])\n",
    "# Mean-T\n",
    "    gbto = pnw_average(obs[dset]['tas'],'lat','lon')\n",
    "    dmeanto[dset] = pnw_average(obs[dset]['tas'].groupby('time.year').mean(dim='time',skipna=True),'lat','lon')\n",
    "    dmeanto[dset] = dmeanto[dset].mean()\n",
    "\n",
    "# Seasonal Amplitude-T\n",
    "    it=0\n",
    "    dsampto[dset] = np.zeros(nyr)\n",
    "    for year in oyears[dset]:\n",
    "        tmp = gbto.sel(time=slice(str(year)+'-01-01',str(year)+'-12-31'))\n",
    "        dsampto[dset][it] = tmp.max()-tmp.min()\n",
    "        it+=1\n",
    "    dsampto[dset] = dsampto[dset].mean()\n",
    "\n",
    "# Mean-P\n",
    "    tmp = obs[dset]['pr']/10\n",
    "    gbpo = pnw_average(tmp,'lat','lon')\n",
    "    dmeanpo[dset] = pnw_average(tmp.groupby('time.year').sum(dim='time',skipna=False),'lat','lon')\n",
    "    dmeanpo[dset] = dmeanpo[dset].mean()\n",
    "\n",
    "# Seasonal Amplitude-P\n",
    "    it=0\n",
    "    dsamppo[dset] = np.zeros(nyr)\n",
    "    for year in oyears[dset]:\n",
    "        tmp = gbpo.sel(time=slice(str(year)+'-01-01',str(year)+'-12-31'))\n",
    "        dsamppo[dset][it] = tmp.max()-tmp.min()\n",
    "        it+=1\n",
    "    dsamppo[dset] = dsamppo[dset].mean()\n",
    "\n",
    "# Nino3.4/ELI - variable Anomalies DJF\n",
    "    pnwlat = slice(35,55)\n",
    "    pnwlon = slice(230,258)\n",
    "\n",
    "    ddjf_corrs_obs[dset] = {}\n",
    "    ddjf_corrs_obs[dset]['n34pr'] = xs.pearson_r(oseasvars[dset]['DJF']['n34'],oseasvars[dset]['DJF']['pranom'].sel(lat=pnwlat,lon=pnwlon,drop=True),dim='time')\n",
    "    ddjf_corrs_obs[dset]['elipr'] = xs.pearson_r(oseasvars[dset]['DJF']['eli'],oseasvars[dset]['DJF']['pranom'].sel(lat=pnwlat,lon=pnwlon,drop=True),dim='time')\n",
    "    ddjf_corrs_obs[dset]['n34t'] = xs.pearson_r(oseasvars[dset]['DJF']['n34'],oseasvars[dset]['DJF']['tasanom'].sel(lat=pnwlat,lon=pnwlon,drop=True),dim='time')\n",
    "    ddjf_corrs_obs[dset]['elit'] = xs.pearson_r(oseasvars[dset]['DJF']['eli'],oseasvars[dset]['DJF']['tasanom'].sel(lat=pnwlat,lon=pnwlon,drop=True),dim='time')\n",
    "    if dset in ['CRU','UDel']:\n",
    "        tmp = obs[dset]['pr'].sel(time=slice('1901-01-01','2014-12-30'),drop=True)/10\n",
    "        ptmpo = pnw_average(tmp.groupby('time.year').sum(dim='time',skipna=False),'lat','lon')\n",
    "        ttmpo = pnw_average((obs[dset]['tas'].sel(time=slice('1901-01-01','2014-12-30'),drop=True)).groupby('time.year').mean(dim='time',skipna=True),'lat','lon')\n",
    "\n",
    "        dptrendo[dset] = (xs.linslope(ptmpo['year'], ptmpo, dim='year')*100).data\n",
    "        dttrendo[dset] = (xs.linslope(ttmpo['year'], ttmpo, dim='year')*100).data\n",
    "\n",
    "# Reference grid that all models and obs are regridded to for comparison\n",
    "dummy1deg = xr.Dataset(\n",
    "        data_vars = dict(\n",
    "    ),\n",
    "    coords = dict(\n",
    "        lon = (['lon'], np.arange(236,253)),\n",
    "        lat = (['lat'], np.arange(41,50)),\n",
    "    ),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c478a7bb-d2e4-4349-81f8-e6526daebeed",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute observational mean metrics\n",
    "\n",
    "dco = {}\n",
    "meanto = np.zeros(1)\n",
    "sampto = np.zeros(1)\n",
    "meanpo = np.zeros(1)\n",
    "samppo = np.zeros(1)\n",
    "\n",
    "ptrendo = np.zeros(1)\n",
    "ttrendo = np.zeros(1)\n",
    "\n",
    "nd = len(odsets)\n",
    "ndt=0\n",
    "for met in ['n34pr','elipr','n34t','elit']:\n",
    "    dco[met] = np.zeros(1)\n",
    "for dset in odsets:\n",
    "    meanto = meanto + dmeanto[dset].data\n",
    "    sampto = sampto + dsampto[dset].data\n",
    "    meanpo = meanpo + dmeanpo[dset].data\n",
    "    samppo = samppo + dsamppo[dset].data\n",
    "    for met in ['n34pr','elipr','n34t','elit']:\n",
    "        dco[met] = dco[met] + ddjf_corrs_obs[dset][met].data\n",
    "    if dset in ['CRU','UDel']:\n",
    "        ndt += 1\n",
    "        ptrendo = ptrendo + dptrendo[dset]\n",
    "        ttrendo = ttrendo + dttrendo[dset]\n",
    "\n",
    "meanto = meanto/nd\n",
    "sampto = sampto/nd\n",
    "meanpo = meanpo/nd\n",
    "samppo = samppo/nd\n",
    "ptrendo = ptrendo/ndt\n",
    "ttrendo = ttrendo/ndt\n",
    "for met in ['n34pr','elipr','n34t','elit']:\n",
    "    dco[met] = dco[met]/nd\n",
    "\n",
    "seasvars_obs = {}\n",
    "avgr = {}\n",
    "avgr['DJF'] = [1,2,12]\n",
    "avgr['MAM'] = [3,4,5]\n",
    "avgr['JJA'] = [6,7,8]\n",
    "avgr['SON'] = [9,10,11]\n",
    "\n",
    "dseasvars_obs = {}\n",
    "dseas_scorrs = {}\n",
    "dseas_stdevs = {}\n",
    "for seas in ['DJF','MAM','JJA','SON']:\n",
    "    seasvars_obs[seas] = {}\n",
    "    dseasvars_obs[seas] = {}\n",
    "\n",
    "    i = 0\n",
    "    for dset in odsets:\n",
    "        dseasvars_obs[seas][dset] = {}\n",
    "        dseasvars_obs[seas][dset]['tas'] = obs[dset]['tas'].sel(lat=pnwlat,lon=pnwlon,drop=True).groupby('time.month').mean(dim='time').sel(month=avgr[seas],drop=True).mean(dim='month')\n",
    "        dseasvars_obs[seas][dset]['pr'] = obs[dset]['pr'].sel(lat=pnwlat,lon=pnwlon,drop=True).groupby('time.month').mean(dim='time').sel(month=avgr[seas],drop=True).mean(dim='month')\n",
    "        if i==0:\n",
    "            seasvars_obs[seas]['tas'] = dseasvars_obs[seas][dset]['tas']\n",
    "            seasvars_obs[seas]['pr'] = dseasvars_obs[seas][dset]['pr']\n",
    "            i += 1\n",
    "        else:\n",
    "            seasvars_obs[seas]['tas'] = seasvars_obs[seas]['tas'] + dseasvars_obs[seas][dset]['tas']\n",
    "            seasvars_obs[seas]['pr'] = seasvars_obs[seas]['pr'] + dseasvars_obs[seas][dset]['pr']\n",
    "    seasvars_obs[seas]['tas'] = seasvars_obs[seas]['tas']/nd\n",
    "    seasvars_obs[seas]['pr'] = seasvars_obs[seas]['pr']/nd\n",
    "\n",
    "    dseas_scorrs[seas] = np.full((len(odsets),len(odsets),2),np.nan)\n",
    "    dseas_stdevs[seas] = np.full((len(odsets),len(odsets),2),np.nan)\n",
    "    for i in range(len(odsets)):\n",
    "        dset = odsets[i]\n",
    "        for j in range(len(odsets)):\n",
    "            dset2 = odsets[j]\n",
    "            if j==i:\n",
    "                continue\n",
    "            else:\n",
    "                dseas_scorrs[seas][i,j,0] = xs.pearson_r(dseasvars_obs[seas][dset]['tas'],dseasvars_obs[seas][dset2]['tas'],dim=['lat','lon'],skipna=True).data\n",
    "                dseas_scorrs[seas][i,j,1] = xs.pearson_r(dseasvars_obs[seas][dset]['pr'],dseasvars_obs[seas][dset2]['pr'],dim=['lat','lon'],skipna=True).data\n",
    "                dseas_stdevs[seas][i,j,0] = np.nanstd(dseasvars_obs[seas][dset]['tas'].data)/np.nanstd(dseasvars_obs[seas][dset2]['tas'].data)\n",
    "                dseas_stdevs[seas][i,j,1] = np.nanstd(dseasvars_obs[seas][dset]['pr'].data)/np.nanstd(dseasvars_obs[seas][dset2]['pr'].data)\n",
    "    dseas_scorrs[seas] = np.nanmean(dseas_scorrs[seas],axis=1)\n",
    "    dseas_stdevs[seas] = np.nanmean(dseas_stdevs[seas],axis=1)\n",
    "djf_corrs_obs = xr.Dataset(\n",
    "        data_vars = dict(\n",
    "        n34pr = (['lat','lon'], dco['n34pr']),\n",
    "        elipr = (['lat','lon'], dco['elipr']),\n",
    "        n34t = (['lat','lon'], dco['n34t']),\n",
    "        elit = (['lat','lon'], dco['elit']),\n",
    "    ),\n",
    "    coords = dict(\n",
    "        lon = (['lon'], seasvars_obs['DJF']['tas']['lon'].data),\n",
    "        lat = (['lat'], seasvars_obs['DJF']['tas']['lat'].data),\n",
    "    ),\n",
    ")\n",
    "ensomets = ['n34pr','elipr','n34t','elit']\n",
    "obs_enso_corrs = {}\n",
    "for imet in range(4):\n",
    "    met = ensomets[imet]\n",
    "    obs_enso_corrs[met] = np.full((6,6),np.nan)\n",
    "    for i in range(len(odsets)):\n",
    "        dset = odsets[i]\n",
    "        for j in range(len(odsets)):\n",
    "            dset2 = odsets[j]\n",
    "            if i==j:\n",
    "                continue\n",
    "            else:\n",
    "                obs_enso_corrs[met][i,j] = xs.pearson_r(ddjf_corrs_obs[dset][met],ddjf_corrs_obs[dset2][met],skipna=True).data\n",
    "    obs_enso_corrs[met] = np.nanmean(obs_enso_corrs[met],axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e587ca4-019a-4adf-a7e6-7f13df1de2de",
   "metadata": {},
   "outputs": [],
   "source": [
    "nmet = 28\n",
    "\n",
    "obsmet = np.full((nd,nmet),np.nan)\n",
    "for i in range(nd):\n",
    "    dset = odsets[i]\n",
    "    \n",
    "## Mean-T, Mean-P, Seasonal Amplitude-T, Seasonal Amplitude-P\n",
    "    j=0\n",
    "    obsmet[i,j] = dmeanto[dset]-meanto.item()\n",
    "    j+=1\n",
    "    obsmet[i,j] = dmeanpo[dset]-meanpo.item()\n",
    "    j+=1\n",
    "    obsmet[i,j] = dsampto[dset]\n",
    "    j+=1\n",
    "    obsmet[i,j] = dsamppo[dset]\n",
    "    j+=1\n",
    "    \n",
    "#P-Trend and T-Trend only computed for CRU and UDel\n",
    "    if dset in ['CRU','UDel']:\n",
    "        obsmet[i,j] = dttrendo[dset]\n",
    "        j+=1\n",
    "        obsmet[i,j] = dptrendo[dset]\n",
    "        j+=1\n",
    "    else:\n",
    "        obsmet[i,j] = np.nan\n",
    "        j+=1\n",
    "        obsmet[i,j] = np.nan\n",
    "        j+=1\n",
    "## DJF Spatial Correlation with Obs for Nino3.4 and ELI for T and P\n",
    "    obsmet[i,j] = obs_enso_corrs['n34pr'][i]\n",
    "    j+=1\n",
    "    obsmet[i,j] = obs_enso_corrs['elipr'][i]\n",
    "    j+=1\n",
    "    obsmet[i,j] = obs_enso_corrs['n34t'][i]\n",
    "    j+=1\n",
    "    obsmet[i,j] = obs_enso_corrs['elit'][i]\n",
    "    j+=1\n",
    "## Seasonal Spatial Correlation for T and P with Obs\n",
    "    obsmet[i,j] = dseas_scorrs['DJF'][i,0]\n",
    "    j+=1\n",
    "    obsmet[i,j] = dseas_scorrs['MAM'][i,0]\n",
    "    j+=1\n",
    "    obsmet[i,j] = dseas_scorrs['JJA'][i,0]\n",
    "    j+=1\n",
    "    obsmet[i,j] = dseas_scorrs['SON'][i,0]\n",
    "    j+=1\n",
    "    obsmet[i,j] = dseas_scorrs['DJF'][i,1]\n",
    "    j+=1\n",
    "    obsmet[i,j] = dseas_scorrs['MAM'][i,1]\n",
    "    j+=1\n",
    "    obsmet[i,j] = dseas_scorrs['JJA'][i,1]\n",
    "    j+=1\n",
    "    obsmet[i,j] = dseas_scorrs['SON'][i,1]\n",
    "    j+=1\n",
    "## Seasonal Spatial Standard Deviation for T and P with Obs\n",
    "    obsmet[i,j] = dseas_stdevs['DJF'][i,0]\n",
    "    j+=1\n",
    "    obsmet[i,j] = dseas_stdevs['MAM'][i,0]\n",
    "    j+=1\n",
    "    obsmet[i,j] = dseas_stdevs['JJA'][i,0]\n",
    "    j+=1\n",
    "    obsmet[i,j] = dseas_stdevs['SON'][i,0]\n",
    "    j+=1\n",
    "    obsmet[i,j] = dseas_stdevs['DJF'][i,1]\n",
    "    j+=1\n",
    "    obsmet[i,j] = dseas_stdevs['MAM'][i,1]\n",
    "    j+=1\n",
    "    obsmet[i,j] = dseas_stdevs['JJA'][i,1]\n",
    "    j+=1\n",
    "    obsmet[i,j] = dseas_stdevs['SON'][i,1]\n",
    "print(j+1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52c62638-2b11-402a-9ac3-5b4612bea63a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib.patches import Polygon\n",
    "fig = plt.figure()\n",
    "ll_lat = latty[0]\n",
    "ur_lat = latty[-1]\n",
    "ll_lon = lonny[0]\n",
    "ur_lon = lonny[-1]\n",
    "px,py = np.meshgrid(lonny,latty)\n",
    "m = Basemap(resolution='i',projection='merc',llcrnrlat=ll_lat,llcrnrlon=ll_lon,\n",
    "            urcrnrlat=ur_lat,urcrnrlon=ur_lon)\n",
    "x,y = m(px,py)\n",
    "lats = [ 40.5, 49.5, 49.5, 40.5 ]\n",
    "lons = [ 234.5, 234.5, 253.5, 253.5 ]\n",
    "xr, yr = m( lons, lats )\n",
    "xryr = zip(xr,yr)\n",
    "poly = Polygon( list(xryr),facecolor='none',edgecolor='red',linewidth = 3, alpha=1.0 )\n",
    "plt.gca().add_patch(poly)\n",
    "m.drawcoastlines()\n",
    "m.drawstates()\n",
    "m.drawcountries()\n",
    "m.drawparallels(np.arange(int(ll_lat),int(ur_lat),5.),labels=[1,0,0,0])\n",
    "m.drawmeridians(np.arange(int(ll_lon-1),int(ur_lon-1),10.),labels=[0,0,0,1])\n",
    "fig.patch.set_facecolor('w')\n",
    "plt.title('PNW Evaluation Domain')\n",
    "plt.savefig('/glade/work/nlybarger/data/hydromet/ESM_eval_semifinal_plots/final/domain.png',dpi=300,bbox_inches='tight',facecolor='w')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69a3b63c-8389-4691-a4df-84478547ffaa",
   "metadata": {},
   "source": [
    "### Metric Checklist\n",
    "- MeanT\n",
    "- MeanP\n",
    "- SeasonAmpT\n",
    "- SeasonAmpP\n",
    "- DJF_ELI_med_bias\n",
    "- DJF_ELI_Levene\n",
    "- SpaceCor_N34_P\n",
    "- SpaceCor_ELI_P\n",
    "- SpaceCor_N34_T\n",
    "- SpaceCor_ELI_T\n",
    "- SpaceCor - MMMT\n",
    "- SpaceCor - MMMP (expanded domain)\n",
    "- SpaceSD - MMMT\n",
    "- SpaceSD - MMMP (expanded domain)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4100afb4-7fbf-481d-afd9-682550f6c710",
   "metadata": {},
   "outputs": [],
   "source": [
    "# output directory\n",
    "diro = '/glade/work/nlybarger/data/hydromet/cmip6_metrics/PNW/'\n",
    "\n",
    "im = 0\n",
    "for mod in models:\n",
    "    if os.path.exists(diro + mod + '.cmip6.metrics.PNW.nc'):\n",
    "        print('Metrics already computed for ' + mod + '.  Advancing.')\n",
    "        continue\n",
    "    nvar = len(variants[mod])\n",
    "    im+=1\n",
    "    print(str(im) + ': Beginning computation of verification metrics for model: ' + mod)\n",
    "    if mod == 'NorCPM1':\n",
    "        print(mod + ' raises an error from the ESMF regridder.  Skipping.')\n",
    "        continue\n",
    "    latstr,lonstr,latdim,londim = coordnames(nci[mod])\n",
    "\n",
    "# converts tas to degrees C if in K\n",
    "    if nci[mod]['tas'].max() > 100.:\n",
    "        nci[mod]['tas'] = nci[mod]['tas']-273.15\n",
    "    month_length = nci[mod].time.dt.days_in_month\n",
    "    if firstrun == True:\n",
    "        nci[mod]['pr'] = nci[mod]['pr']*month_length\n",
    "# Mean-T\n",
    "    gbt = pnw_average(nci[mod]['tas'],latstr,lonstr)\n",
    "    meant = pnw_average(nci[mod]['tas'].groupby('time.year').mean(dim='time',skipna=True),latstr,lonstr)\n",
    "    meant = meant.mean(dim='year',skipna=True)\n",
    "\n",
    "# Seasonal Amplitude-T\n",
    "    years = list(nci[mod].groupby('time.year').groups)\n",
    "    sampt = np.zeros((nvar,len(years)))\n",
    "    it=0\n",
    "    for year in years:\n",
    "        tmp=gbt.sel(time=slice(str(year)+'-01-01',str(year)+'-12-30'))\n",
    "        for iv in range(nvar):\n",
    "            tmp0 = tmp.sel(ens=variants[mod][iv])\n",
    "            sampt[iv,it] = tmp0.max()-tmp0.min()\n",
    "        it+=1\n",
    "    sampt = sampt.mean(axis=1)\n",
    "\n",
    "# Mean-P\n",
    "    tmp = nci[mod]['pr']/10  # convert units from mm/day to cm/mo a la Rupp et al\n",
    "    gbp = pnw_average(tmp,latstr,lonstr)\n",
    "    meanp = pnw_average(tmp.groupby('time.year').sum(dim='time',skipna=False),latstr,lonstr)\n",
    "    meanp = meanp.mean(dim='year',skipna=True)\n",
    "        \n",
    "# Seasonal Amplitude-P\n",
    "    years = list(nci[mod].groupby('time.year').groups)\n",
    "    sampp = np.zeros((nvar,len(years)))\n",
    "    it=0\n",
    "    for year in years:\n",
    "        tmp=gbp.sel(time=slice(str(year)+'-01-01',str(year)+'-12-30'))\n",
    "        for iv in range(nvar):\n",
    "            tmp0 = tmp.sel(ens=variants[mod][iv])\n",
    "            sampp[iv,it] = tmp0.max()-tmp0.min()\n",
    "        it+=1\n",
    "    sampp = sampp.mean(axis=1)\n",
    "\n",
    "# ELI median bias and Levene's statistic\n",
    "    seasvars = seasonal_avg_vars(nci[mod],mod,latstr,lonstr,False)\n",
    "    elimed = np.zeros(nvar)\n",
    "    levstat = np.zeros(nvar)\n",
    "    for iv in range(nvar):\n",
    "        elimed[iv] = np.median(seasvars['DJF']['eli'].sel(ens=variants[mod][iv]))\n",
    "        levstat[iv],_ = scipy.stats.levene(seasvars['DJF']['eli'].sel(ens=variants[mod][iv]).data,\n",
    "                                                oseasvars['CRU']['DJF']['eli'].data,center='median')\n",
    "        \n",
    "# Correlations between ELI/N34 and pr/tas\n",
    "    djf_corrs_mod = {}\n",
    "    djf_corrs_mod['n34pr'] = xs.pearson_r(seasvars['DJF']['n34'],\n",
    "                                          seasvars['DJF']['pranom'].sel(lat=pnwlat,lon=pnwlon,drop=True),dim='time')\n",
    "    djf_corrs_mod['elipr'] = xs.pearson_r(seasvars['DJF']['eli'],\n",
    "                                          seasvars['DJF']['pranom'].sel(lat=pnwlat,lon=pnwlon,drop=True),dim='time')\n",
    "    djf_corrs_mod['n34t'] = xs.pearson_r(seasvars['DJF']['n34'],\n",
    "                                         seasvars['DJF']['tasanom'].sel(lat=pnwlat,lon=pnwlon,drop=True),dim='time')\n",
    "    djf_corrs_mod['elit'] = xs.pearson_r(seasvars['DJF']['eli'],\n",
    "                                         seasvars['DJF']['tasanom'].sel(lat=pnwlat,lon=pnwlon,drop=True),dim='time')\n",
    "    djf_mets = list(djf_corrs_mod.keys())\n",
    "\n",
    "    regridder_pnw = xesmf.Regridder(seasvars['DJF'].sel(ens=variants[mod][0],lat=pnwlat,lon=pnwlon,drop=True),\n",
    "                                    dummy1deg,'bilinear')\n",
    "    for met in djf_mets:\n",
    "        djf_corrs_mod[met] = regridder_pnw(djf_corrs_mod[met])\n",
    "        djf_corrs_mod[met] = djf_corrs_mod[met].transpose('ens', 'lat', 'lon')\n",
    "    \n",
    "    djf_enso_scorrs = np.zeros((len(djf_mets),nvar,nd))\n",
    "    for imet in range(4):\n",
    "        met = djf_mets[imet]\n",
    "        for ie in range(nvar):\n",
    "            tmp = djf_corrs_mod[met].sel(ens=variants[mod][ie],drop=True)\n",
    "            for idset in range(len(odsets)):\n",
    "                djf_enso_scorrs[imet,ie,idset] = xs.pearson_r(tmp,ddjf_corrs_obs[dset][met],skipna=True).data\n",
    "    djf_enso_scorrs = np.nanmean(djf_enso_scorrs,axis=2)\n",
    "    \n",
    "# Mean Seasonal average spatial correlation and standard deviation\n",
    "    seaslist = ['DJF','MAM','JJA','SON']\n",
    "    seas_scorrs = np.zeros((2,4,nvar,nd))\n",
    "    seas_sstdev = np.zeros((2,4,nvar,nd))\n",
    "    seastas = {}\n",
    "    seaspr = {}\n",
    "    for isea in range(4):\n",
    "        seas=seaslist[isea]\n",
    "        seastas[seas] = nci[mod]['tas'].sel(lat=pnwlat,lon=pnwlon,drop=True).groupby('time.month').mean(dim='time').sel(month=avgr[seas],drop=True).mean(dim='month')\n",
    "        seastas[seas] = regridder_pnw(seastas[seas])\n",
    "        tmpobs = seasvars_obs[seas]['tas']\n",
    "        for ie in range(nvar):\n",
    "            seastas[seas][ie,:,:] = xr.where(~np.isnan(tmpobs),seastas[seas][ie,:,:],np.nan)\n",
    "        seaspr[seas]  = nci[mod]['pr'].sel(lat=pnwlat,lon=pnwlon,drop=True).groupby('time.month').mean(dim='time').sel(month=avgr[seas],drop=True).mean(dim='month')\n",
    "        seaspr[seas] = regridder_pnw(seaspr[seas])\n",
    "        tmpobs = seasvars_obs[seas]['pr']\n",
    "        for ie in range(nvar):\n",
    "            seaspr[seas][ie,:,:] = xr.where(~np.isnan(tmpobs),seaspr[seas][ie,:,:],np.nan)\n",
    "        for ie in range(nvar):\n",
    "            for idset in range(len(odsets)):\n",
    "                seas_scorrs[0,isea,ie,idset] = xs.pearson_r(seastas[seas].sel(ens=variants[mod][ie],drop=True),dseasvars_obs[seas][odsets[idset]]['tas'],dim=['lat','lon'],skipna=True).data\n",
    "                seas_sstdev[0,isea,ie,idset] = np.nanstd(seastas[seas].sel(ens=variants[mod][ie],drop=True).data)/np.nanstd(dseasvars_obs[seas][odsets[idset]]['tas'].data)\n",
    "        for ie in range(nvar):\n",
    "            for idset in range(len(odsets)):\n",
    "                seas_scorrs[1,isea,ie,idset] = xs.pearson_r(seaspr[seas].sel(ens=variants[mod][ie],drop=True),dseasvars_obs[seas][odsets[idset]]['pr'],dim=['lat','lon'],skipna=True).data\n",
    "                seas_sstdev[1,isea,ie,idset] = np.nanstd(seaspr[seas].sel(ens=variants[mod][ie],drop=True).data)/np.nanstd(dseasvars_obs[seas][odsets[idset]]['pr'].data)\n",
    "    seas_scorrs = np.nanmean(seas_scorrs,axis=3)\n",
    "    seas_sstdev = np.nanmean(seas_sstdev,axis=3)\n",
    "\n",
    "    tmp = nci[mod]['pr']/10\n",
    "    nens = len(nci[mod]['ens'])\n",
    "    pt = pnw_average(tmp.sel(time=slice('1901-01-01','2014-12-30'),drop=True),'lat','lon')\n",
    "    pt = pt.groupby('time.year').sum(dim='time',skipna=False)\n",
    "    tt = pnw_average(nci[mod]['tas'].sel(time=slice('1901-01-01','2014-12-30'),drop=True),'lat','lon')\n",
    "    tt = tt.groupby('time.year').mean(dim='time')\n",
    "    ptrend = xs.linslope(pt['year'],pt,dim='year',skipna=False)*100\n",
    "    ttrend = xs.linslope(tt['year'],tt,dim='year',skipna=False)*100\n",
    "\n",
    "    metrics = xr.Dataset(\n",
    "                data_vars = dict(\n",
    "                    meant      = (['ens'], meant.data),\n",
    "                    sampt      = (['ens'], sampt),\n",
    "                    meanp      = (['ens'], meanp.data),\n",
    "                    sampp      = (['ens'], sampp),\n",
    "                    ttrend     = (['ens'], ttrend.data),\n",
    "                    ptrend     = (['ens'], ptrend.data),\n",
    "\n",
    "                    elimed = (['ens'], elimed),\n",
    "                    eli_djf = (['ens','time'], seasvars['DJF']['eli'].data),\n",
    "                    levstat    = (['ens'], levstat),\n",
    "\n",
    "                    n34pr_rdjf = (['ens'], djf_enso_scorrs[0,:]),\n",
    "                    elipr_rdjf = (['ens'], djf_enso_scorrs[1,:]),\n",
    "                    n34t_rdjf  = (['ens'], djf_enso_scorrs[2,:]),\n",
    "                    elit_rdjf  = (['ens'], djf_enso_scorrs[3,:]),\n",
    "\n",
    "                    djf_t_r    = (['ens'], seas_scorrs[0,0,:]),\n",
    "                    djf_pr_r   = (['ens'], seas_scorrs[1,0,:]),\n",
    "                    mam_t_r    = (['ens'], seas_scorrs[0,1,:]),\n",
    "                    mam_pr_r   = (['ens'], seas_scorrs[1,1,:]),\n",
    "                    jja_t_r    = (['ens'], seas_scorrs[0,2,:]),\n",
    "                    jja_pr_r   = (['ens'], seas_scorrs[1,2,:]),\n",
    "                    son_t_r    = (['ens'], seas_scorrs[0,3,:]),\n",
    "                    son_pr_r   = (['ens'], seas_scorrs[1,3,:]),\n",
    "\n",
    "                    djf_t_sd   = (['ens'], seas_sstdev[0,0,:]),\n",
    "                    djf_pr_sd  = (['ens'], seas_sstdev[1,0,:]),\n",
    "                    mam_t_sd   = (['ens'], seas_sstdev[0,1,:]),\n",
    "                    mam_pr_sd  = (['ens'], seas_sstdev[1,1,:]),\n",
    "                    jja_t_sd   = (['ens'], seas_sstdev[0,2,:]),\n",
    "                    jja_pr_sd  = (['ens'], seas_sstdev[1,2,:]),\n",
    "                    son_t_sd   = (['ens'], seas_sstdev[0,3,:]),\n",
    "                    son_pr_sd  = (['ens'], seas_sstdev[1,3,:]),\n",
    "                    \n",
    "                    djf_t  = (['ens','lat','lon'], seastas['DJF'].data),\n",
    "                    djf_pr = (['ens','lat','lon'], seaspr['DJF'].data),\n",
    "                    mam_t  = (['ens','lat','lon'], seastas['MAM'].data),\n",
    "                    mam_pr = (['ens','lat','lon'], seaspr['MAM'].data),\n",
    "                    jja_t  = (['ens','lat','lon'], seastas['JJA'].data),\n",
    "                    jja_pr = (['ens','lat','lon'], seaspr['JJA'].data),\n",
    "                    son_t  = (['ens','lat','lon'], seastas['SON'].data),\n",
    "                    son_pr = (['ens','lat','lon'], seaspr['SON'].data),\n",
    "                    \n",
    "                    djf_n34_pr_scorr = (['ens','lat','lon'], djf_corrs_mod['n34pr'].data),\n",
    "                    djf_eli_pr_scorr = (['ens','lat','lon'], djf_corrs_mod['elipr'].data),\n",
    "                    djf_n34_t_scorr  = (['ens','lat','lon'], djf_corrs_mod['n34t'].data),\n",
    "                    djf_eli_t_scorr  = (['ens','lat','lon'], djf_corrs_mod['elit'].data),\n",
    "                    \n",
    "                ),\n",
    "                coords = dict(\n",
    "                    ens  = (['ens'], nci[mod]['ens'].data),\n",
    "                    time = (['time'], seasvars['DJF']['time'].data),\n",
    "                    lat  = (['lat'], dummy1deg['lat'].data),\n",
    "                    lon  = (['lon'], dummy1deg['lon'].data),\n",
    "                ),\n",
    "                attrs = dict(\n",
    "                    description=('CMIP6 metrics for model: ' + mod))\n",
    "            )\n",
    "    metrics.to_netcdf(diro + mod + '.cmip6.metrics.PNW.nc',mode='w')\n",
    "    \n",
    "    del seasvars\n",
    "    del metrics\n",
    "    del seaspr\n",
    "    del seastas\n",
    "firstrun = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4191a10e-e021-41da-a46b-6489938b6813",
   "metadata": {},
   "outputs": [],
   "source": [
    "if 'NorCPM1' in models:\n",
    "    models.remove('NorCPM1')\n",
    "\n",
    "diro = '/glade/work/nlybarger/data/hydromet/cmip6_metrics/PNW/'\n",
    "metrics = {}\n",
    "for mod in models:\n",
    "    tmpfil = diro + mod + '.cmip6.metrics.PNW.nc'\n",
    "    if os.path.exists(diro + mod + '.cmip6.metrics.PNW.nc'):\n",
    "        metrics[mod] = xr.open_dataset(tmpfil,engine='netcdf4')\n",
    "    else:\n",
    "        continue\n",
    "\n",
    "# Other MCM-UA-1-0 ensemble members show odd behavior\n",
    "metrics['MCM-UA-1-0'] = metrics['MCM-UA-1-0'].isel(ens=[1])\n",
    "\n",
    "# These models are removed from the list for various reasons\n",
    "# Bizarre precipitation calculations, likely incorrect units in database\n",
    "remmodels = ['KIOST-ESM','CIESM','E3SM-1-1','NorCPM1']\n",
    "for mod in remmodels:\n",
    "    if mod in list(metrics.keys()):\n",
    "        metrics.pop(mod)\n",
    "models = sorted(list(metrics.keys()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e426e61a-6779-4d3f-88a1-2d1da363e8ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Number of metrics\n",
    "nmet = 26\n",
    "\n",
    "# Max number of ensemble members\n",
    "nens = 72\n",
    "\n",
    "# Number of models to be evaluated\n",
    "nmod = len(models)\n",
    "\n",
    "# Raw metric values\n",
    "modmet = np.full((nmod,nmet,nens),np.nan)\n",
    "\n",
    "# Absolute error matrix as compared to obs mean (or perfect correlation/stdev)\n",
    "errs = np.full((nmod,nmet,nens),np.nan)\n",
    "\n",
    "for i in range(nmod):\n",
    "## Mean-T, Mean-P, Seasonal Amplitude-T, Seasonal Amplitude-P\n",
    "    nensmod = len(metrics[models[i]]['ens'])\n",
    "    j=0\n",
    "    modmet[i,j,:nensmod] = metrics[models[i]]['meant']\n",
    "    errs[i,j,:nensmod] = abs(modmet[i,j,:nensmod]-meanto.item())\n",
    "    j+=1\n",
    "    modmet[i,j,:nensmod] = metrics[models[i]]['meanp']\n",
    "    errs[i,j,:nensmod] = abs(modmet[i,j,:nensmod]-meanpo.item())\n",
    "    j+=1\n",
    "    modmet[i,j,:nensmod] = metrics[models[i]]['sampt']\n",
    "    errs[i,j,:nensmod] = abs(modmet[i,j,:nensmod]-sampto)\n",
    "    j+=1\n",
    "    modmet[i,j,:nensmod] = metrics[models[i]]['sampp']\n",
    "    errs[i,j,:nensmod] = abs(modmet[i,j,:nensmod]-samppo)\n",
    "    j+=1\n",
    "    modmet[i,j,:nensmod] = metrics[models[i]]['ttrend']\n",
    "    errs[i,j,:nensmod] = abs(modmet[i,j,:nensmod]-ttrendo)\n",
    "    j+=1\n",
    "    modmet[i,j,:nensmod] = metrics[models[i]]['ptrend']\n",
    "    errs[i,j,:nensmod] = abs(modmet[i,j,:nensmod]-ptrendo)\n",
    "    j+=1\n",
    "## DJF Spatial Correlation with Obs for Nino3.4 and ELI for T and P\n",
    "    modmet[i,j,:nensmod] = metrics[models[i]]['n34pr_rdjf']\n",
    "    errs[i,j,:nensmod] = abs(1-modmet[i,j,:nensmod])\n",
    "    j+=1\n",
    "    modmet[i,j,:nensmod] = metrics[models[i]]['elipr_rdjf']\n",
    "    errs[i,j,:nensmod] = abs(1-modmet[i,j,:nensmod])\n",
    "    j+=1\n",
    "    modmet[i,j,:nensmod] = metrics[models[i]]['n34t_rdjf']\n",
    "    errs[i,j,:nensmod] = abs(1-modmet[i,j,:nensmod])\n",
    "    j+=1\n",
    "    modmet[i,j,:nensmod] = metrics[models[i]]['elit_rdjf']\n",
    "    errs[i,j,:nensmod] = abs(1-modmet[i,j,:nensmod])\n",
    "    j+=1\n",
    "## Seasonal Spatial Correlation for T and P with Obs\n",
    "    modmet[i,j,:nensmod] = metrics[models[i]]['djf_t_r']\n",
    "    errs[i,j,:nensmod] = abs(1-modmet[i,j,:nensmod])\n",
    "    j+=1\n",
    "    modmet[i,j,:nensmod] = metrics[models[i]]['mam_t_r']\n",
    "    errs[i,j,:nensmod] = abs(1-modmet[i,j,:nensmod])\n",
    "    j+=1\n",
    "    modmet[i,j,:nensmod] = metrics[models[i]]['jja_t_r']\n",
    "    errs[i,j,:nensmod] = abs(1-modmet[i,j,:nensmod])\n",
    "    j+=1\n",
    "    modmet[i,j,:nensmod] = metrics[models[i]]['son_t_r']\n",
    "    errs[i,j,:nensmod] = abs(1-modmet[i,j,:nensmod])\n",
    "    j+=1\n",
    "    modmet[i,j,:nensmod] = metrics[models[i]]['djf_pr_r']\n",
    "    errs[i,j,:nensmod] = abs(1-modmet[i,j,:nensmod])\n",
    "    j+=1\n",
    "    modmet[i,j,:nensmod] = metrics[models[i]]['mam_pr_r']\n",
    "    errs[i,j,:nensmod] = abs(1-modmet[i,j,:nensmod])\n",
    "    j+=1\n",
    "    modmet[i,j,:nensmod] = metrics[models[i]]['jja_pr_r']\n",
    "    errs[i,j,:nensmod] = abs(1-modmet[i,j,:nensmod])\n",
    "    j+=1\n",
    "    modmet[i,j,:nensmod] = metrics[models[i]]['son_pr_r']\n",
    "    errs[i,j,:nensmod] = abs(1-modmet[i,j,:nensmod])\n",
    "    j+=1\n",
    "## Seasonal Spatial Standard Deviation for T and P with Obs\n",
    "    modmet[i,j,:nensmod] = metrics[models[i]]['djf_t_sd']\n",
    "    errs[i,j,:nensmod] = abs(1-modmet[i,j,:nensmod])\n",
    "    j+=1\n",
    "    modmet[i,j,:nensmod] = metrics[models[i]]['mam_t_sd']\n",
    "    errs[i,j,:nensmod] = abs(1-modmet[i,j,:nensmod])\n",
    "    j+=1\n",
    "    modmet[i,j,:nensmod] = metrics[models[i]]['jja_t_sd']\n",
    "    errs[i,j,:nensmod] = abs(1-modmet[i,j,:nensmod])\n",
    "    j+=1\n",
    "    modmet[i,j,:nensmod] = metrics[models[i]]['son_t_sd']\n",
    "    errs[i,j,:nensmod] = abs(1-modmet[i,j,:nensmod])\n",
    "    j+=1\n",
    "    modmet[i,j,:nensmod] = metrics[models[i]]['djf_pr_sd']\n",
    "    errs[i,j,:nensmod] = abs(1-modmet[i,j,:nensmod])\n",
    "    j+=1\n",
    "    modmet[i,j,:nensmod] = metrics[models[i]]['mam_pr_sd']\n",
    "    errs[i,j,:nensmod] = abs(1-modmet[i,j,:nensmod])\n",
    "    j+=1\n",
    "    modmet[i,j,:nensmod] = metrics[models[i]]['jja_pr_sd']\n",
    "    errs[i,j,:nensmod] = abs(1-modmet[i,j,:nensmod])\n",
    "    j+=1\n",
    "    modmet[i,j,:nensmod] = metrics[models[i]]['son_pr_sd']\n",
    "    errs[i,j,:nensmod] = abs(1-modmet[i,j,:nensmod])\n",
    "print(j+1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c20ae69-f4ea-4778-9c69-11805c39aaf3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setting up metric plot titles\n",
    "metric_titles = ['Mean-T\\n°C','Mean-P\\ncm/yr','SeasAmp-T\\n°C','SeasAmp-P\\ncm/mo',\n",
    "                 'Trend-T\\n°C/century','Trend-P\\ncm/century',\n",
    "                 'Nino3.4-pr r','ELI-pr r','Nino3.4-T r','ELI-T r',\n",
    "                 'DJF-T','MAM-T','JJA-T','SON-T',\n",
    "                 'DJF-P','MAM-P','JJA-P','SON-P',\n",
    "                 'DJF-T','MAM-T','JJA-T','SON-T',\n",
    "                 'DJF-P','MAM-P','JJA-P','SON-P']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93a80072-0392-42e5-8c24-31610cb0bd80",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plotting PNW metric comparisons\n",
    "\n",
    "tlinst = 'c-'\n",
    "modmet_ensm = np.nanmean(modmet,axis=2)\n",
    "mask = ~np.isnan(modmet_ensm)\n",
    "filty = [d[m] for d, m in zip(modmet_ensm.T, mask.T)]\n",
    "i=0\n",
    "xran = [.05,1.95]\n",
    "fig,axs = plt.subplots(4,8,figsize=(11,10),sharex='all')\n",
    "for ax in axs.flat:\n",
    "    if i in [6,7,12,13,14,15]:\n",
    "        ax.remove()\n",
    "        i+=1\n",
    "        continue\n",
    "    if i<=5:\n",
    "        ax.boxplot(filty[i],whiskerprops=dict(linestyle='-',linewidth=3),\n",
    "                                    capprops=dict(linewidth=3),\n",
    "                                    boxprops=dict(linewidth=3),\n",
    "                                    medianprops=dict(linewidth=3),\n",
    "                                    flierprops=dict(marker='o',markerfacecolor='r',markeredgecolor='k',linestyle='none'),\n",
    "                                    widths=1.5)\n",
    "    elif (i>=8) and (i<=11):\n",
    "        ax.boxplot(filty[i-2],whiskerprops=dict(linestyle='-',linewidth=3),\n",
    "                                    capprops=dict(linewidth=3),\n",
    "                                    boxprops=dict(linewidth=3),\n",
    "                                    medianprops=dict(linewidth=3),\n",
    "                                    flierprops=dict(marker='o',markerfacecolor='r',markeredgecolor='k',linestyle='none'),\n",
    "                                    widths=1.5)\n",
    "    else:\n",
    "        ax.boxplot(filty[i-6],whiskerprops=dict(linestyle='-',linewidth=3),\n",
    "                                    capprops=dict(linewidth=3),\n",
    "                                    boxprops=dict(linewidth=3),\n",
    "                                    medianprops=dict(linewidth=3),\n",
    "                                    flierprops=dict(marker='o',markerfacecolor='r',markeredgecolor='k',linestyle='none'),\n",
    "                                    widths=1.5)\n",
    "        \n",
    "    #===================================================\n",
    "    \n",
    "    if i == 0:\n",
    "        ax.plot(xran,[meanto.item(),meanto.item()],tlinst,linewidth=3,zorder=-1)\n",
    "        #ax.set_ylabel('°C',fontsize=14)\n",
    "        ax.set_ylim([0.,13.])\n",
    "        ax.set_yticks([0,3,6,9,12])\n",
    "    elif i == 1:\n",
    "        ax.plot(xran,[meanpo.item(),meanpo.item()],tlinst,linewidth=3,zorder=-1)\n",
    "        #ax.set_ylabel('cm/yr',fontsize=14)\n",
    "        ax.set_ylim([14.,118.])\n",
    "        ax.set_yticks([25,50,75,100])\n",
    "        ax.yaxis.get_majorticklabels()[3].set_x(.065)\n",
    "        ax.yaxis.set_label_coords(-.27, .5)\n",
    "    elif i == 2:\n",
    "        ax.plot(xran,[sampto,sampto],tlinst,linewidth=3,zorder=-1)\n",
    "        #ax.set_ylabel('°C',fontsize=14)\n",
    "        ax.set_ylim([16.6,32.])\n",
    "        ax.set_yticks([20,24,28,32])\n",
    "    elif i == 3:\n",
    "        ax.plot(xran,[samppo,samppo],tlinst,linewidth=3,zorder=-1)\n",
    "        #ax.set_ylabel('cm/mo',fontsize=14)\n",
    "        ax.set_ylim([2.,17.])\n",
    "        ax.set_yticks([5,10,15])\n",
    "    elif i == 4:\n",
    "        ax.plot(xran,[ttrendo,ttrendo],tlinst,linewidth=3,zorder=-1)\n",
    "        #ax.set_ylabel('°C/century',fontsize=14)\n",
    "        ax.set_ylim([-0.89,2.])\n",
    "        ax.set_yticks([-0.,1,2])\n",
    "    elif i == 5:\n",
    "        ax.plot(xran,[ptrendo,ptrendo],tlinst,linewidth=3,zorder=-1)\n",
    "        #ax.set_ylabel('cm/century', fontsize=14)\n",
    "        ax.set_ylim([-7.,11.82])\n",
    "        ax.set_yticks([-5,0,5,10])\n",
    "        ax.yaxis.set_label_coords(-.27, .5)\n",
    "    elif i in (np.arange(8,12)):\n",
    "        ax.plot(xran,[1,1],tlinst,linewidth=3,zorder=-1)\n",
    "        ax.set_ylim([-1.1,1.2])\n",
    "        if i>8:\n",
    "            ax.set_yticklabels([])\n",
    "    elif i in (np.arange(16,24)):\n",
    "        ax.plot(xran,[1,1],tlinst,linewidth=3,zorder=-1)\n",
    "        ax.set_ylim([0.0,1.1])\n",
    "        if i>16:\n",
    "            ax.set_yticklabels([])\n",
    "        if i==16:\n",
    "            ax.set_ylabel('SpaceCorr',fontsize=14)\n",
    "    elif i in (np.arange(24,32)):\n",
    "        ax.plot(xran,[1,1],tlinst,linewidth=3,zorder=-1)\n",
    "        ax.set_ylim([0.0,2.75])\n",
    "        ax.set_yticks([0,0.5,1,1.5,2,2.5])\n",
    "        if i>24:\n",
    "            ax.set_yticklabels([])\n",
    "        if i==24:\n",
    "            ax.set_ylabel('SpaceSD',fontsize=14)\n",
    "    else:\n",
    "        ax.plot(xran,[1,1],tlinst,linewidth=3,zorder=-1)\n",
    "        \n",
    "    #===================================================\n",
    "    \n",
    "    if i<=5:\n",
    "        ax.scatter(np.ones(modmet_ensm.shape[0]),modmet_ensm[:,i],c='k',s=30)\n",
    "    elif (i>=8) and (i<=11):\n",
    "        ax.scatter(np.ones(modmet_ensm.shape[0]),modmet_ensm[:,i-2],c='k',s=30)\n",
    "    else:\n",
    "        ax.scatter(np.ones(modmet_ensm.shape[0]),modmet_ensm[:,i-6],c='k',s=30)\n",
    "        \n",
    "    #===================================================\n",
    "    \n",
    "    if i == 0:\n",
    "        ax.scatter(np.ones(obsmet.shape[0]),obsmet[:,i]+meanto.item(),c='cyan',s=75,edgecolors='k',zorder=5)\n",
    "    elif i == 1:\n",
    "        ax.scatter(np.ones(obsmet.shape[0]),obsmet[:,i]+meanpo.item(),c='cyan',s=75,edgecolors='k',zorder=5)\n",
    "    elif i <= 5:\n",
    "        ax.scatter(np.ones(obsmet.shape[0]),obsmet[:,i],c='cyan',s=75,edgecolors='k',zorder=5)\n",
    "    elif (i>=8) and (i<=11):\n",
    "        ax.scatter(np.ones(obsmet.shape[0]),obsmet[:,i-2],c='cyan',s=75,edgecolors='k',zorder=5)\n",
    "    else:\n",
    "        ax.scatter(np.ones(obsmet.shape[0]),obsmet[:,i-6],c='cyan',s=75,edgecolors='k',zorder=5)\n",
    "        \n",
    "    #===================================================\n",
    "    \n",
    "    metric_title_size = 13\n",
    "    if i<=5:\n",
    "        ax.set_title(metric_titles[i],fontsize=metric_title_size)\n",
    "        \n",
    "    elif (i>=8) and (i<=11):\n",
    "        ax.set_title(metric_titles[i-2],fontsize=metric_title_size)\n",
    "    else:\n",
    "        ax.set_title(metric_titles[i-6],fontsize=metric_title_size)\n",
    "        \n",
    "    #===================================================\n",
    "    \n",
    "    ax.set_xlim(xran[0],xran[1])\n",
    "    ax.set_xticks([])\n",
    "    ax.tick_params(axis='both', which='major', labelsize=12)\n",
    "    i += 1\n",
    "    \n",
    "fig.subplots_adjust(left=0.125,right=0.9,wspace=0.55,hspace=0.25)\n",
    "plt.suptitle('Pacific Northwest Metrics',fontsize=22,y=0.98)\n",
    "plt.savefig('/glade/u/home/nlybarger/CMIP6_1900-2014.PNW.metricarray.png',dpi=450,bbox_inches='tight',facecolor='w')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "030dee3a-a296-429f-8826-632a85e89892",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Quick and dirty spatial resolution output\n",
    "for mod in models:\n",
    "    if mod in list(nci.keys()):\n",
    "        print(str(round(abs((nci[mod]['lon'][0]-nci[mod]['lon'][1]).data),2)) +' x '+ str(round(abs((nci[mod]['lat'][0]-nci[mod]['lat'][1]).data),2)))\n",
    "    else:\n",
    "        print('uh what')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mypy",
   "language": "python",
   "name": "mypy"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
